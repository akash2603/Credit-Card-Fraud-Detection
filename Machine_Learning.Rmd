---
title: "Credit Card Fraud Detection"
author: "Akash Barnwal"
date: "April 11, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r}
library(ggplot2)
library(readr)
library(caret)
library(e1071)
library(randomForest)
library(ROSE)
```


```{r}
# Reading the data file from system

setwd("E:/DataScience/Semester 2/Project")

raw_data <- read.csv("creditcard.csv", header=T, na.strings=c(""))

# The Initial Step in Building Successful Predictive Analytics Solutions


# 1) Depicting a random sample of 100 records from the file to understand the data
raw_data_sample <- raw_data[sample(1:nrow(raw_data), 100, replace=FALSE),]

# Checking no of missing values in data
sapply(raw_data,function(x) sum(is.na(x)))
```
Result:
We can see that there is no missing values in the data


Checking number of unique values in the data
```{r}
sapply(raw_data, function(x) length(unique(x)))
```


Removing the missing values
```{r}
raw_data <- na.omit(raw_data)
raw_data <- raw_data[,c(1,31,30,2:29)]
```


Observations:

1) The data is positively skewed with fraud trasactions very less as compared to the non fraud ones. 
2) The total number of fraud transactions are 492 and non fraud ones are 284315. This sort of skewness makes sense since no of fraud data has to be less as compared to the non fraud data.

Inference drawn:

1) Given such imbalance in the data, an algorithm which doesn't do any analysis will give an accuracy of 99.828%. Hence accuracy is not the correct measure of correctness while classifying transactions as fraud and non fraud.

2) Time features shows the chronological order of the transaction hence its not a significant feature to be kept. 

```{r}
raw_data <- raw_data[, !(names(raw_data) == "Time")]
```

Converting class into factor for better analysis
```{r}
raw_data$Class <-as.factor(raw_data$Class)
```


```{r}
library(caret)

data=createDataPartition(raw_data$Class,p=.7,list=FALSE)

train_data=raw_data[data,]

test_data=raw_data[-data,]
```

```{r}
table(train_data$Class)
```
The  no of 0's and 1's in train class are 199021 and 345.

```{r}
table(test_data$Class)
```
The  no of 0's and 1's in train class are 85294 and 147 .

```{r}
no_fraud_rows <- nrow(test_data[test_data$Class == 1,])

rowsTotal <- nrow(raw_data)

LenTestdata <- rowsTotal - nrow(train_data)

nonFraudRows <- LenTestdata - no_fraud_rows
```

Accuracy of a model that predicts all the cases as non-frauds without Modelling

```{r}
accuracyBase <- nonFraudRows/LenTestdata
accuracyBase

# Using Binomial Logistic Regression Algorithm implementation of the R Package.

# Getting the test data and training data for current iteration

# Modelling the data 
model <- glm(formula = Class~., family=binomial(link="logit"), data=train_data)
summary(model)
```

Inferences:

1) According to the model output, we can see that the significant variables are V4, V8, V10, V13, V14, V20, V21, V22, V27 and v28. 

2) The negative coefficient for this predictor suggests that all other variables being equal, the variables with negative coefficient is less likely to have fraud values.
 
 
Classifying the test data on the basis of the above model:
 
In the steps above, we briefly evaluated the fitting of the model, now we would like to see how the model is doing when predicting y on a new set of data. 

By setting the parameter type='response', R will output probabilities in the form of P(y=1|X).

```{r}
p <- predict(model, newdata=subset(test_data), type="response")

library(ROCR)
# Predicting the cutoff probabilities for the predicted values
pr <- prediction(p, test_data$Class)



perf <- performance(pr, "tpr", "fpr")
plot(perf)

# Now we can run the anova() function on the model to analyze the table of deviance
anova(model, test="Chisq")
```
Inference:
  
The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better.
 
A large p-value here indicates that the model without the variable explains more or less the same amount of variation.

While no exact equivalent to the R2 of linear regression exists, the McFadden R2 index can be used to assess the model fit.
```{r}
library(pscl)
pR2(model)
```


Compute area under the ROC curve (Output of AUC is cutoff-independent)
```{r}
auc <- performance(pr, measure = "auc")

auc_logistic <- auc@y.values[[1]]
auc_logistic
```
Inference:

An accurancy of 98.70 shows that the model is very good.

```{r}
prec_recall <- data.frame(p, test_data$Class)

library(PRROC)
prc <- pr.curve(prec_recall[prec_recall$test_data.Class == 1,]$p, prec_recall[prec_recall$test_data.Class == 0,]$p)
prc
```


Getting all the values from the prediction model for true positive, true negative, false positive,
false negative

```{r}
cutoffs <- pr@cutoffs[[1]]
tp <- pr@tp[[1]]
tn <- pr@tn[[1]]
fn <- pr@fn[[1]]
fp <- pr@fp[[1]]

maxTpIndex <- 0
maxSpecSensIndex <- 1
maxSensSpecThreshold <- 0
midCutoffIndex <- 0

# finding the cutoff probability by iterating through the cutoff values in the predicted outcome.

for(i in seq_along(cutoffs)) {
  sensitivity <- tp[i]/no_fraud_rows
  specificity <- tn[i]/nonFraudRows
  sensSpecThreshold <- sensitivity + specificity
  
  if(sensSpecThreshold > maxSensSpecThreshold) 
    
    {
    maxSensSpecThreshold <- sensSpecThreshold
    maxSpecSensIndex <- i
    }
  
  if(sensitivity == 1 && maxTpIndex == 0){
    maxTpIndex <- i
  } 
  
  if(cutoffs[i][[1]] < 0.5 && midCutoffIndex == 0) {
    midCutoffIndex <- i
  }
}

# Graph data for plotting the sensitivity and specificity curves and saving the plot

graph_x = cutoffs
graph_y1 = tp/no_fraud_rows
graph_y2 = tn/nonFraudRows

par(mar=c(6,5,5,6)+0.5)
plot(graph_x,graph_y1,type="l",col="red",yaxt="n",xlab="",ylab="", main="V1-V28")
axis(2)
par(new=TRUE)
plot(graph_x, graph_y2,type="l",col="green",xaxt="n",yaxt="n",xlab="",ylab="")
axis(4)
mtext("Specificity",side=4,line=3)
mtext("Sensitivity",side=2,line=3)
mtext("Cutoff",side=1,line=3)
legend("right",col=c("red","green"),lty=1,legend=c("Sensitivity","Specificity"))
```

Testing the model using concordance- discordance pair test

```{r}
 concordance<-function(model){   
       # Get all actual observations and their fitted values into a frame   
       fitted<-data.frame(cbind(model$y,model$fitted.values))   
         colnames(fitted)<-c('respvar','score')   
           # Subset only ones   
           ones<-fitted[fitted[,1]==1,]   
             # Subset only zeros   
             zeros<-fitted[fitted[,1]==0,]   
            
               pairs_tested<-0   
                 conc<-0   
                   disc<-0   
                     ties<-0   
                    
                       # Get the values in a for-loop   
                       for(i in 1:nrow(ones))   
                       {   
                             for(j in 1:nrow(zeros))   
                             {   
                                   pairs_tested<-pairs_tested+1   
                                     if(ones[i,2]>zeros[j,2]) {conc<-conc+1}   
                                     else if(ones[i,2]==zeros[j,2]){ties<-ties+1}   
                                     else {disc<-disc+1}   
                                   }   
                           }   
                       # Calculate concordance, discordance and ties   
                       concordance<-conc/pairs_tested   
                         discordance<-disc/pairs_tested   
                           ties_perc<-ties/pairs_tested   
                             return(list("Concordance"=concordance,   
                                                              "Discordance"=discordance,   
                                                              "Tied"=ties_perc,   
                                                              "Pairs"=pairs_tested))   
                           }   
  
   concordance(model)   
```  

Down sampling the positive data samples to avoid data imbalance
```{r}

data_balanced_under <- ovun.sample(Class ~ ., data = train_data, method = "under", N = 600, seed = 1)$data
table(data_balanced_under$Class)
```


## Prediction using different Models

SVM Model
```{r}
model_svm <- train(Class~.,data=data_balanced_under,method="svmRadial",trControl=trainControl(method='cv'))

pred_svm <- predict(model_svm, test_data)
cm_svm <- confusionMatrix(pred_svm,test_data$Class, positive = "0")
cm_svm
acc_svm <- cm_svm$overall['Accuracy']
acc_svm
```

Random Forest
```{r}
model_rf <- train(Class~.,data=data_balanced_under,method="ranger",trControl=trainControl(method='cv'),preProcess = c("center","scale"))
pred_rf <- predict(model_rf, test_data)
cm_rf <-confusionMatrix(pred_rf,test_data$Class, positive = "0")
acc_rf <- cm_rf$overall['Accuracy']
acc_rf
```



KNN
```{r}
model_knn <- train(Class~.,data=data_balanced_under,method="knn",trControl=trainControl(method='cv'),preProcess = c("center","scale"))
pred_knn <- predict(model_knn, test_data)
cm_knn <- confusionMatrix(pred_knn,test_data$Class, positive = "0")
acc_knn <- cm_knn$overall['Accuracy']
cm_knn
```


Neural net
```{r}
model_nn <- train(Class~.,data=data_balanced_under,method="nnet",trControl=trainControl(method='cv'))
pred_nn <- predict(model_nn, test_data)
cm_nn <- confusionMatrix(pred_nn,test_data$Class, positive = "0")
acc_nn <- cm_nn$overall['Accuracy']
cm_nn
```

```{r}
accuracy_list = c(auc_logistic, acc_svm, acc_rf, acc_knn, acc_nn)
accuracy_list_names <- c("Logistic", "SVM","RandomForest","KNN","Neural Net")

# Plotting the accuracy of different models model_svm
colors <- c("red", "darkolivegreen4","orange","brown","cadetblue1")
barplot(accuracy_list, names.arg = accuracy_list_names, col = colors,ylim=c(0,1.1),xlab = "Model",ylab="Accuracy")
```

We can see that the best performing model is the Logistic Regression.



